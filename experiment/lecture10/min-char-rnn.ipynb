{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal character-level Vanilla RNN model. \n",
    "\n",
    "Originally written by Andrej Karpathy (@karpathy)\n",
    "See: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "Some details were tweaked by me to fit into Python3 and I add some comments to make it easy to understand. Also predict() was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1557 characters, 58 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.'% (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code, each variable represents...\n",
    "\n",
    "- h : the output at each time step\n",
    "- y : final scores \n",
    "\n",
    "and we feed input as a one-hot vector 'cause Wxh is of shape (..., vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # dy: dL/df \n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        # dWhy: dL/dWhy_ij = dL/df_i * hs[t]_j\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        # dby: dL/df * 1\n",
    "        dby += dy\n",
    "        # dh: dL/dhs[t]_i = (\\sum_{j=0}^{j<vocab_size} Why_ji * dy_j) + (gradient flow from the next layer)\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        # dhraw := dL/dh'\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        # dbh: dL/dh' * 1\n",
    "        dbh += dhraw\n",
    "        # dWxh, dWhh: similar to dWhy\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        # dhnext: dL/dh[t-1] whose calculation is similar to dh\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "# This function is called when you make a prediction.\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model made of softmax values\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 101.410282\n",
      "iter 200, loss: 98.115715\n",
      "iter 400, loss: 90.028837\n",
      "iter 600, loss: 81.686995\n",
      "iter 800, loss: 73.898432\n",
      "iter 1000, loss: 66.715745\n",
      "iter 1200, loss: 60.215532\n",
      "iter 1400, loss: 54.405039\n",
      "iter 1600, loss: 49.246946\n",
      "iter 1800, loss: 44.736955\n",
      "iter 2000, loss: 40.778748\n",
      "iter 2200, loss: 37.319262\n",
      "iter 2400, loss: 34.175291\n",
      "iter 2600, loss: 31.408708\n",
      "iter 2800, loss: 28.934205\n",
      "iter 3000, loss: 26.729003\n",
      "iter 3200, loss: 24.750248\n",
      "iter 3400, loss: 23.238156\n",
      "iter 3600, loss: 21.763793\n",
      "iter 3800, loss: 20.395916\n",
      "iter 4000, loss: 19.164656\n",
      "iter 4200, loss: 18.015122\n",
      "iter 4400, loss: 16.990547\n",
      "iter 4600, loss: 16.113048\n",
      "iter 4800, loss: 15.331188\n",
      "iter 5000, loss: 14.532514\n",
      "iter 5200, loss: 13.717501\n",
      "iter 5400, loss: 13.074833\n",
      "iter 5600, loss: 12.682093\n",
      "iter 5800, loss: 12.075024\n",
      "iter 6000, loss: 11.510866\n",
      "iter 6200, loss: 10.962611\n",
      "iter 6400, loss: 10.612205\n",
      "iter 6600, loss: 10.243169\n",
      "iter 6800, loss: 9.838026\n",
      "iter 7000, loss: 9.446223\n",
      "iter 7200, loss: 9.019860\n",
      "iter 7400, loss: 8.772565\n",
      "iter 7600, loss: 8.772819\n",
      "iter 7800, loss: 8.524559\n",
      "iter 8000, loss: 8.254838\n",
      "iter 8200, loss: 7.945181\n",
      "iter 8400, loss: 7.658560\n",
      "iter 8600, loss: 7.386654\n",
      "iter 8800, loss: 7.141807\n",
      "iter 9000, loss: 6.917767\n",
      "iter 9200, loss: 6.840487\n",
      "iter 9400, loss: 6.733644\n",
      "iter 9600, loss: 6.506729\n",
      "iter 9800, loss: 6.426306\n",
      "iter 10000, loss: 6.259557\n",
      "iter 10200, loss: 6.102187\n",
      "iter 10400, loss: 5.943815\n",
      "iter 10600, loss: 5.890823\n",
      "iter 10800, loss: 5.755910\n",
      "iter 11000, loss: 5.776186\n",
      "iter 11200, loss: 5.590456\n",
      "iter 11400, loss: 5.393073\n",
      "iter 11600, loss: 5.247502\n",
      "iter 11800, loss: 5.356543\n",
      "iter 12000, loss: 5.296544\n",
      "iter 12200, loss: 5.121587\n",
      "iter 12400, loss: 4.958817\n",
      "iter 12600, loss: 4.858982\n",
      "iter 12800, loss: 4.798108\n",
      "iter 13000, loss: 4.808854\n",
      "iter 13200, loss: 4.742429\n",
      "iter 13400, loss: 4.582070\n",
      "iter 13600, loss: 4.477281\n",
      "iter 13800, loss: 4.345052\n",
      "iter 14000, loss: 4.204899\n",
      "iter 14200, loss: 4.059348\n",
      "iter 14400, loss: 4.125151\n",
      "iter 14600, loss: 4.169121\n",
      "iter 14800, loss: 4.182510\n",
      "iter 15000, loss: 4.164691\n",
      "iter 15200, loss: 4.130264\n",
      "iter 15400, loss: 3.968449\n",
      "iter 15600, loss: 3.810705\n",
      "iter 15800, loss: 3.669446\n",
      "iter 16000, loss: 4.036857\n",
      "iter 16200, loss: 3.927106\n",
      "iter 16400, loss: 3.772963\n",
      "iter 16600, loss: 3.645868\n",
      "iter 16800, loss: 3.723221\n",
      "iter 17000, loss: 3.620554\n",
      "iter 17200, loss: 3.499916\n",
      "iter 17400, loss: 3.440637\n",
      "iter 17600, loss: 3.391822\n",
      "iter 17800, loss: 3.326078\n",
      "iter 18000, loss: 3.497719\n",
      "iter 18200, loss: 3.430032\n",
      "iter 18400, loss: 3.305150\n",
      "iter 18600, loss: 3.328978\n",
      "iter 18800, loss: 3.241216\n",
      "iter 19000, loss: 3.134134\n",
      "iter 19200, loss: 3.043430\n",
      "iter 19400, loss: 2.975724\n",
      "iter 19600, loss: 3.454089\n",
      "iter 19800, loss: 3.469555\n",
      "iter 20000, loss: 3.381714\n",
      "iter 20200, loss: 3.232816\n",
      "iter 20400, loss: 3.087231\n",
      "iter 20600, loss: 3.060995\n",
      "iter 20800, loss: 3.026510\n",
      "iter 21000, loss: 2.906490\n",
      "iter 21200, loss: 2.841724\n",
      "iter 21400, loss: 2.874670\n",
      "iter 21600, loss: 2.781059\n",
      "iter 21800, loss: 2.671763\n",
      "iter 22000, loss: 2.779210\n",
      "iter 22200, loss: 3.073963\n",
      "iter 22400, loss: 3.044489\n",
      "iter 22600, loss: 2.931793\n",
      "iter 22800, loss: 2.804787\n",
      "iter 23000, loss: 2.679499\n",
      "iter 23200, loss: 2.557775\n",
      "iter 23400, loss: 2.551469\n",
      "iter 23600, loss: 2.930656\n",
      "iter 23800, loss: 2.964012\n",
      "iter 24000, loss: 2.843581\n",
      "iter 24200, loss: 2.733165\n",
      "iter 24400, loss: 2.592296\n",
      "iter 24600, loss: 2.496759\n",
      "iter 24800, loss: 2.487861\n",
      "iter 25000, loss: 2.431765\n",
      "iter 25200, loss: 2.335446\n",
      "iter 25400, loss: 2.298284\n",
      "iter 25600, loss: 2.466688\n",
      "iter 25800, loss: 2.424137\n",
      "iter 26000, loss: 2.324515\n",
      "iter 26200, loss: 2.307277\n",
      "iter 26400, loss: 2.248831\n",
      "iter 26600, loss: 2.188180\n",
      "iter 26800, loss: 2.109616\n",
      "iter 27000, loss: 2.144155\n",
      "iter 27200, loss: 2.094026\n",
      "iter 27400, loss: 2.041672\n",
      "iter 27600, loss: 1.975746\n",
      "iter 27800, loss: 1.938914\n",
      "iter 28000, loss: 2.195357\n",
      "iter 28200, loss: 2.149110\n",
      "iter 28400, loss: 2.089302\n",
      "iter 28600, loss: 2.022887\n",
      "iter 28800, loss: 1.962810\n",
      "iter 29000, loss: 2.022033\n",
      "iter 29200, loss: 2.049502\n",
      "iter 29400, loss: 1.971067\n",
      "iter 29600, loss: 1.908138\n",
      "iter 29800, loss: 1.864459\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "iteration = 30000\n",
    "\n",
    "for i in range(iteration):\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    # if n % 100 == 0:\n",
    "    #     sample_ix = sample(hprev, inputs[0], 200)\n",
    "    #     txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    #     print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 200 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Citation from the original author's blog:\n",
    "        http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "At test time, we feed a character into the RNN and \n",
    "get a distribution over what characters are likely to come next. \n",
    "We sample from this distribution, and feed it right back in to get the next letter.\n",
    "Repeat this process and you are sampling text! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(char, length=2000):\n",
    "    idx = char_to_ix[char]\n",
    "    hprev = np.zeros((hidden_size,1))\n",
    "    sample_ix = sample(hprev, inputs[0], length)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    return txt\n",
    "\n",
    "def output_to_file(txt):\n",
    "    f = open('output.txt', 'w')\n",
    "    f.write(txt)\n",
    "\n",
    "prediction = predict('#')\n",
    "output_to_file(prediction)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
